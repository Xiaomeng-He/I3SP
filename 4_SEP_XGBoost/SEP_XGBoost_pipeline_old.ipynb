{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPIC 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_len = 88\n",
    "num_act = 29\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for training an XGBoost model to predict the next activity label\n",
    "param_act = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': num_act,\n",
    "        'max_depth': 8,\n",
    "        'min_child_weight': 9.74462727911892,\n",
    "        'subsample': 0.9618841777880232,\n",
    "        'colsample_bytree': 0.6582241534026132,\n",
    "        'lambda': 8.002051385874758,\n",
    "        'eta': 0.10167697633502167\n",
    "    }\n",
    "\n",
    "# hyperparameters for training an XGBoost model to predict the next timestamp\n",
    "param_time = {\n",
    "        'objective': 'reg:absoluteerror',\n",
    "        'eval_metric': 'mae',\n",
    "        'max_depth': 12,\n",
    "        'min_child_weight': 12.337223244031959,\n",
    "        'subsample': 0.8453782420681207,\n",
    "        'colsample_bytree': 0.9033521246902182,\n",
    "        'lambda': 8.3746228993637,\n",
    "        'eta': 0.2196528959499405\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for training an XGBoost model to predict the next activity label\n",
    "param_act = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': num_act,\n",
    "        'max_depth': 10,\n",
    "        'min_child_weight': 4.261024311224565,\n",
    "        'subsample': 0.9804376496746907,\n",
    "        'colsample_bytree': 0.9187764362444105,\n",
    "        'lambda': 7.912072508972737,\n",
    "        'eta': 0.10394052247291184\n",
    "    }\n",
    "\n",
    "# hyperparameters for training an XGBoost model to predict the next timestamp\n",
    "param_time = {\n",
    "        'objective': 'reg:absoluteerror',\n",
    "        'eval_metric': 'mae',\n",
    "        'max_depth': 11,\n",
    "        'min_child_weight': 11.667738097521406,\n",
    "        'subsample': 0.9240363624981516,\n",
    "        'colsample_bytree': 0.9785011357939114,\n",
    "        'lambda': 3.1663267935061397,\n",
    "        'eta': 0.11949621033263035\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPIC 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_len = 14\n",
    "num_act = 40\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for training an XGBoost model to predict the next activity label\n",
    "param_act = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': num_act,\n",
    "        'max_depth': 7,\n",
    "        'min_child_weight': 2.8049924942971702,\n",
    "        'subsample': 0.6638855846724679,\n",
    "        'colsample_bytree': 0.7864021562769484,\n",
    "        'lambda': 7.926181642148752,\n",
    "        'eta': 0.14555480478649802\n",
    "    }\n",
    "\n",
    "# hyperparameters for training an XGBoost model to predict the next timestamp\n",
    "param_time = {\n",
    "        'objective': 'reg:absoluteerror',\n",
    "        'eval_metric': 'mae',\n",
    "        'max_depth': 12,\n",
    "        'min_child_weight': 17.011718425833887,\n",
    "        'subsample': 0.8548789057622406,\n",
    "        'colsample_bytree': 0.6225615621294496,\n",
    "        'lambda': 3.8661288379363032,\n",
    "        'eta': 0.2469307468511154\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for training an XGBoost model to predict the next activity label\n",
    "param_act = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': num_act,\n",
    "        'max_depth': 10,\n",
    "        'min_child_weight': 4.261024311224565,\n",
    "        'subsample': 0.9804376496746907,\n",
    "        'colsample_bytree': 0.9187764362444105,\n",
    "        'lambda': 7.912072508972737,\n",
    "        'eta': 0.10394052247291184\n",
    "    }\n",
    "\n",
    "# hyperparameters for training an XGBoost model to predict the next timestamp\n",
    "param_time = {\n",
    "        'objective': 'reg:absoluteerror',\n",
    "        'eval_metric': 'mae',\n",
    "        'max_depth': 12,\n",
    "        'min_child_weight': 8.53429129870081,\n",
    "        'subsample': 0.7912897584098408,\n",
    "        'colsample_bytree': 0.9122487127320722,\n",
    "        'lambda': 3.4505487528134626,\n",
    "        'eta': 0.14425392950846272\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file path\n",
    "train_log_prefix_tensor_path = '.../train_log_prefix_tensor.pt'\n",
    "train_trace_prefix_tensor_path = '.../train_trace_prefix_tensor.pt'\n",
    "train_suffix_act_tensor_path = '.../train_suffix_act_tensor.pt'\n",
    "train_suffix_time_tensor_path = '.../train_suffix_time_tensor.pt'\n",
    "\n",
    "val_log_prefix_tensor_path = '.../val_log_prefix_tensor.pt'\n",
    "val_trace_prefix_tensor_path = '.../val_trace_prefix_tensor.pt'\n",
    "val_suffix_act_tensor_path = '.../val_suffix_act_tensor.pt'\n",
    "val_suffix_time_tensor_path = '.../val_suffix_time_tensor.pt'\n",
    "\n",
    "test_log_prefix_tensor_path = '.../test_log_prefix_tensor.pt'\n",
    "test_trace_prefix_tensor_path = '.../test_trace_prefix_tensor.pt'\n",
    "test_suffix_act_tensor_path = '.../test_suffix_act_tensor.pt'\n",
    "test_suffix_time_tensor_path = '.../test_suffix_time_tensor.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get min and max values\n",
    "max_dict = torch.load('.../max_dict.pt')\n",
    "min_dict = torch.load('.../min_dict.pt')\n",
    "\n",
    "min_pre = min_dict['trace_ts_pre']\n",
    "max_pre = max_dict['trace_ts_pre']\n",
    "\n",
    "min_start = min_dict['trace_ts_start']\n",
    "max_start = max_dict['trace_ts_start']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train\n",
    "train_trace_prefix_tensor = torch.load(train_trace_prefix_tensor_path) # shape: (num_samples, trace_prefix_len, num_act + 2)\n",
    "train_trace_prefix_tensor = train_trace_prefix_tensor.reshape(train_trace_prefix_tensor.shape[0], -1) # shape: (num_samples, trace_prefix_len * (num_act + 2))\n",
    "X_train = train_trace_prefix_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val\n",
    "val_trace_prefix_tensor = torch.load(val_trace_prefix_tensor_path) # shape: (num_samples, trace_prefix_len, num_act + 2)\n",
    "val_trace_prefix_tensor = val_trace_prefix_tensor.reshape(val_trace_prefix_tensor.shape[0], -1) # shape: (num_samples, trace_prefix_len * (num_act + 2))\n",
    "X_val = val_trace_prefix_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test\n",
    "test_trace_prefix_tensor = torch.load(test_trace_prefix_tensor_path) # shape: (num_samples, trace_prefix_len, num_act + 2)\n",
    "test_trace_prefix_tensor = test_trace_prefix_tensor.reshape(test_trace_prefix_tensor.shape[0], -1) # shape: (num_samples, trace_prefix_len * (num_act + 2))\n",
    "X_test = test_trace_prefix_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train\n",
    "train_log_prefix_tensor = torch.load(train_log_prefix_tensor_path) # shape: (num_samples, log_prefix_len, num_act + 1)\n",
    "train_log_prefix_tensor = train_log_prefix_tensor.reshape(train_log_prefix_tensor.shape[0], -1) # shape: (num_samples, (log_prefix_len * (num_act + 1))\n",
    "# remove the activity label of the last event in the log prefix, \n",
    "# as it is redundant (already captured in the trace prefix).\n",
    "train_log_prefix_tensor = torch.cat(\n",
    "    (train_log_prefix_tensor[:, :-(num_act+1)], train_log_prefix_tensor[:, -1].unsqueeze(1)),\n",
    "    dim=1)\n",
    "\n",
    "train_trace_prefix_tensor = torch.load(train_trace_prefix_tensor_path) # shape: (num_samples, trace_prefix_len, num_act + 2)\n",
    "train_trace_prefix_tensor = train_trace_prefix_tensor.reshape(train_trace_prefix_tensor.shape[0], -1) # shape: (num_samples, trace_prefix_len * (num_act + 2))\n",
    "\n",
    "X_train_tensor = torch.cat((train_log_prefix_tensor, train_trace_prefix_tensor), -1)\n",
    "X_train = X_train_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val\n",
    "val_log_prefix_tensor = torch.load(val_log_prefix_tensor_path) # shape: (num_samples, log_prefix_len, num_act + 1)\n",
    "val_log_prefix_tensor = val_log_prefix_tensor.reshape(val_log_prefix_tensor.shape[0], -1) # shape: (num_samples, (log_prefix_len * (num_act + 1))\n",
    "# remove the activity label of the last event in the log prefix, \n",
    "# as it is redundant (already captured in the trace prefix).\n",
    "val_log_prefix_tensor = torch.cat(\n",
    "    (val_log_prefix_tensor[:, :-(num_act+1)], val_log_prefix_tensor[:, -1].unsqueeze(1)),\n",
    "    dim=1)\n",
    "\n",
    "val_trace_prefix_tensor = torch.load(val_trace_prefix_tensor_path) # shape: (num_samples, trace_prefix_len, num_act + 2)\n",
    "val_trace_prefix_tensor = val_trace_prefix_tensor.reshape(val_trace_prefix_tensor.shape[0], -1) # shape: (num_samples, trace_prefix_len * (num_act + 2))\n",
    "\n",
    "X_val_tensor = torch.cat((val_log_prefix_tensor, val_trace_prefix_tensor), -1)\n",
    "X_val = X_val_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test\n",
    "test_log_prefix_tensor = torch.load(test_log_prefix_tensor_path) # shape: (num_samples, log_prefix_len, num_act + 1)\n",
    "test_log_prefix_tensor = test_log_prefix_tensor.reshape(test_log_prefix_tensor.shape[0], -1) # shape: (num_samples, (log_prefix_len * (num_act + 1))\n",
    "# remove the activity label of the last event in the log prefix, \n",
    "# as it is redundant (already captured in the trace prefix).\n",
    "test_log_prefix_tensor = torch.cat(\n",
    "    (test_log_prefix_tensor[:, :-(num_act+1)], test_log_prefix_tensor[:, -1].unsqueeze(1)),\n",
    "    dim=1)\n",
    "\n",
    "test_trace_prefix_tensor = torch.load(test_trace_prefix_tensor_path) # shape: (num_samples, trace_prefix_len, num_act + 2)\n",
    "test_trace_prefix_tensor = test_trace_prefix_tensor.reshape(test_trace_prefix_tensor.shape[0], -1) # shape: (num_samples, trace_prefix_len * (num_act + 2))\n",
    "\n",
    "X_test_tensor = torch.cat((test_log_prefix_tensor, test_trace_prefix_tensor), -1)\n",
    "X_test = X_test_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next activity label\n",
    "\n",
    "# y_train_act\n",
    "train_suffix_act_tensor = torch.load(train_suffix_act_tensor_path)\n",
    "y_train_act_tensor = train_suffix_act_tensor[:, 0]\n",
    "y_train_act = y_train_act_tensor.numpy()\n",
    "\n",
    "# y_val_act\n",
    "val_suffix_act_tensor = torch.load(val_suffix_act_tensor_path)\n",
    "y_val_act_tensor = val_suffix_act_tensor[:, 0]\n",
    "y_val_act = y_val_act_tensor.numpy()\n",
    "\n",
    "# y_test_act\n",
    "test_suffix_act_tensor = torch.load(test_suffix_act_tensor_path)\n",
    "y_test_act_tensor = test_suffix_act_tensor[:, 0]\n",
    "y_test_act = y_test_act_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_time\n",
    "train_suffix_time_tensor = torch.load(train_suffix_time_tensor_path)\n",
    "y_train_time_tensor = train_suffix_time_tensor[:, 0]\n",
    "y_train_time = y_train_time_tensor.numpy()\n",
    "\n",
    "# y_val_time\n",
    "val_suffix_time_tensor = torch.load(val_suffix_time_tensor_path)\n",
    "y_val_time_tensor = val_suffix_time_tensor[:, 0]\n",
    "y_val_time = y_val_time_tensor.numpy()\n",
    "\n",
    "# y_test_time\n",
    "test_suffix_time_tensor = torch.load(test_suffix_time_tensor_path)\n",
    "y_test_time_tensor = test_suffix_time_tensor[:, 0]\n",
    "y_test_time = y_test_time_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for next activity label prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train XGBoost\n",
    "dtrain_act = xgb.DMatrix(data=X_train, label=y_train_act)\n",
    "dvalid_act = xgb.DMatrix(data=X_val, label=y_val_act)\n",
    "\n",
    "model_act = xgb.train(param_act, \n",
    "                   dtrain_act, \n",
    "                   evals=[(dtrain_act, 'train'), (dvalid_act, 'validation')], \n",
    "                   num_boost_round = 100000,\n",
    "                   early_stopping_rounds = 20,\n",
    "                   verbose_eval = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate next activity label prediction performance on the test set\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "y_pred_act = model_act.predict(dtest)\n",
    "y_labels = np.argmax(y_pred_act, axis=1)\n",
    "\n",
    "precision = precision_score(y_test_act, y_labels, average='weighted')\n",
    "recall = recall_score(y_test_act, y_labels, average='weighted')\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for next timestamp prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train XGBoost\n",
    "dtrain_time = xgb.DMatrix(data=X_train, label=y_train_time)\n",
    "dvalid_time = xgb.DMatrix(data=X_val, label=y_val_time)\n",
    "\n",
    "model_time= xgb.train(param_time, \n",
    "                   dtrain_time, \n",
    "                   evals=[(dtrain_time, 'train'), (dvalid_time, 'validation')], \n",
    "                   num_boost_round = 100000,\n",
    "                   early_stopping_rounds = 20,\n",
    "                   verbose_eval = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate next timestamp prediction performance on the test set\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "y_pred_time = model_time.predict(dtest)\n",
    "\n",
    "y_pred_time_norm = y_pred_time* (max_pre - min_pre) + min_pre # reverse max-min normalization\n",
    "y_pred_time_log = np.expm1(y_pred_time_norm) # reverse log-transformation\n",
    "\n",
    "y_test_time_norm= y_test_time* (max_pre - min_pre) + min_pre # reverse max-min normalization\n",
    "y_test_time_log = np.expm1(y_test_time_norm) # reverse log-transformation\n",
    "\n",
    "mae = mean_absolute_error(y_test_time_log, y_pred_time_log)\n",
    "msle = mean_squared_error(y_test_time_norm, y_pred_time_norm)\n",
    "print(mae)\n",
    "print(msle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursively generate suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_predictions = np.zeros((X_test.shape[0], suffix_len, num_act))\n",
    "time_predictions = np.zeros((X_test.shape[0], suffix_len, 1))\n",
    "\n",
    "X_test_renew = X_test.copy()\n",
    "\n",
    "for i in range(suffix_len):\n",
    "\n",
    "    dtest = xgb.DMatrix(X_test_renew)\n",
    "\n",
    "    # -- predict TTNE -- #\n",
    "\n",
    "    y_time_pre = model_time.predict(dtest) # shape: (val_size, )\n",
    "    y_time_pre_2d = y_time_pre.reshape(-1, 1) # shape: (val_size, 1)\n",
    "    \n",
    "    # -- derive time since case starts -- #\n",
    "\n",
    "    # get old normalized y_time_start\n",
    "    old_y_time_start = X_test_renew[:, -2] # shape: (val_size, )\n",
    "\n",
    "    # denormalize y_time_start\n",
    "    old_y_time_start = old_y_time_start* (max_start - min_start) + min_start\n",
    "    old_y_time_start = np.expm1(old_y_time_start)\n",
    "\n",
    "    # denormalize predicted TTNE\n",
    "    y_time_pre_denorm = y_time_pre* (max_pre - min_pre) + min_pre\n",
    "    y_time_pre_denorm = np.expm1(y_time_pre_denorm)\n",
    "\n",
    "    # add old_y_time_start and predicted TTNE to get new y_time_start\n",
    "    y_time_start = old_y_time_start + y_time_pre_denorm\n",
    "\n",
    "    # log normalize y_time_start\n",
    "    y_time_start = np.maximum(y_time_start, 0) \n",
    "    y_time_start = np.log1p(y_time_start)\n",
    "    y_time_start = (y_time_start - min_start) / (max_start - min_start)\n",
    "\n",
    "    # -- predict activity label --\n",
    "    y_act = model_act.predict(dtest) # shape: (val_size, num_act)\n",
    "    y_labels = np.argmax(y_act, axis=1) # shape: (val_size,)\n",
    "    y_act_hot = np.eye(num_act)[y_labels] # shape: (val_size, num_act)\n",
    "    y_act_hot[:, 0] = 0 \n",
    "\n",
    "    act_predictions[:, i, :] = y_act\n",
    "    time_predictions[:, i, :] = y_time_pre_2d\n",
    "\n",
    "    # update X_test\n",
    "    X_test_renew[:, -((num_act+2) * suffix_len): -(num_act+2)] = X_test_renew[:, -((num_act+2) * (suffix_len-1)):]\n",
    "    X_test_renew[:, -(num_act+2):-2] = y_act_hot\n",
    "    X_test_renew[:, -2] = y_time_start\n",
    "    X_test_renew[:, -1] = y_time_pre\n",
    "\n",
    "act_predictions = torch.from_numpy(act_predictions)\n",
    "time_predictions = torch.from_numpy(time_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def damerau_levenshtein(list1, \n",
    "                        list2):\n",
    "    \"\"\"\n",
    "    Compute the Damerau-Levenshtein distance between two sequences of activity labels.\n",
    "\n",
    "    The Damerau-Levenshtein distance measures the minimum number of operations \n",
    "    (insertions, deletions, substitutions, and transpositions of adjacent elements) \n",
    "    required to transform one list into the other.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list1 : list\n",
    "        First sequence of activity labels.\n",
    "    list2 : list\n",
    "        Second sequence of activity labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dl_distance : float\n",
    "        The computed Damerau-Levenshtein distance between the two sequences.\n",
    "    \"\"\"\n",
    "    len_1, len_2 = len(list1), len(list2)\n",
    "\n",
    "    dist = [[0 for _ in range(len_2 + 1)] for _ in range(len_1 + 1)]\n",
    "\n",
    "    for i in range(len_1 + 1):\n",
    "        dist[i][0] = i\n",
    "    for j in range(len_2 + 1):\n",
    "        dist[0][j] = j\n",
    "\n",
    "    for i in range(1, len_1 + 1):\n",
    "        for j in range(1, len_2 + 1):\n",
    "            cost = 0 if list1[i - 1] == list2[j - 1] else 1\n",
    "\n",
    "            dist[i][j] = min(\n",
    "                dist[i - 1][j] + 1,    # deletion\n",
    "                dist[i][j - 1] + 1,    # insertion\n",
    "                dist[i - 1][j - 1] + cost  # substitution\n",
    "            )\n",
    "\n",
    "            if i > 1 and j > 1 and list1[i - 1] == list2[j - 2] and list1[i - 2] == list2[j - 1]:\n",
    "                dist[i][j] = min(\n",
    "                    dist[i][j],\n",
    "                    dist[i - 2][j - 2] + cost  # transposition\n",
    "                )\n",
    "\n",
    "    dl_distance = dist[len_1][len_2]\n",
    "\n",
    "    return dl_distance\n",
    "\n",
    "def performance_metrics(act_predictions, \n",
    "                        time_predictions, \n",
    "                        act_suffix,\n",
    "                        time_suffix,\n",
    "                        max_value,\n",
    "                        min_value,\n",
    "                        eoc_index = int(3)):\n",
    "    \"\"\"\n",
    "    Compute three performance metrics for suffix prediction:  \n",
    "    - dl_distance: Normalized Damerau-Levenshtein distance between predicted and \n",
    "      ground truth activity label suffixes.\n",
    "    - mae: Mean Absolute Error between predicted and ground truth TTNE suffixes.\n",
    "    - msle: Mean Squared Logarithmic Error between predicted and ground truth \n",
    "      TTNE suffixes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    act_predictions : torch.Tensor\n",
    "        Activity label suffix predictions.  \n",
    "        Shape: (batch_size, suffix_len, num_act)\n",
    "    time_predictions : torch.Tensor\n",
    "        Timestamp suffix predictions.  \n",
    "        Shape: (batch_size, suffix_len, 1)\n",
    "    act_suffix : torch.Tensor\n",
    "        Ground truth activity label suffix.  \n",
    "        Shape: (batch_size, suffix_len)\n",
    "    time_suffix : torch.Tensor\n",
    "        Ground truth timestamp suffix.  \n",
    "        Shape: (batch_size, suffix_len)\n",
    "    max_value : float\n",
    "        Maximum of the log-normalized TTNE.\n",
    "    min_value : float\n",
    "        Minimum of the log-normalized TTNE.\n",
    "    eoc_index : int\n",
    "        Index representing the End-Of-Case (EOC) token in the activity label \n",
    "        vocabulary. \n",
    "        Default is 3.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dl_distance : float\n",
    "        Average normalized Damerau-Levenshtein distance over the batch.   \n",
    "    mae : float\n",
    "        Average MAE over the batch.\n",
    "    msle : float\n",
    "        Average MSLE over the batch.\n",
    "    \"\"\"\n",
    "    # get batch_size\n",
    "    batch_size = act_predictions.shape[0]\n",
    "\n",
    "    # convert predicted activity label probabilities to lebel indices\n",
    "    act_predictions = act_predictions.argmax(2) # shape: (batch_size, suffix_len)\n",
    "\n",
    "    # reshape time_predictions\n",
    "    time_predictions = time_predictions.squeeze(-1) # shape: (batch_size, suffix_len)\n",
    "\n",
    "    # de-normalize and reverse log transformation of TTNE\n",
    "    time_suffix = time_suffix * (max_value - min_value) + min_value\n",
    "    time_suffix_exp = torch.exp(time_suffix) - 1   \n",
    "    time_predictions = time_predictions * (max_value - min_value) + min_value\n",
    "    time_predictions_exp = torch.exp(time_predictions) - 1\n",
    "\n",
    "    # initialize performance metrics\n",
    "    total_dl_distance, total_mae, total_msle = 0.0, 0.0, 0.0\n",
    "\n",
    "    # define loss functions\n",
    "    time_criterion_1 = nn.L1Loss()\n",
    "    time_criterion_2 = nn.MSELoss()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "\n",
    "        # -- normalized Damerau-Levenshtein distance --\n",
    "\n",
    "        # convert predicted and target activity label sequences to lists, \n",
    "        # truncated at EOC\n",
    "        act_pred_list = []\n",
    "        act_target_list = []\n",
    "\n",
    "        for p in act_predictions[i].tolist():\n",
    "            act_pred_list.append(p)\n",
    "            if p == eoc_index: # stop at EOC token (inclusive)\n",
    "                break    \n",
    "            \n",
    "        for t in act_suffix[i].tolist():\n",
    "            act_target_list.append(t)\n",
    "            if t == eoc_index: # stop at EOC token (inclusive)\n",
    "                break\n",
    "        \n",
    "        # compute normalized Damerau-Levenshtein distance\n",
    "        pred_len, target_len= len(act_pred_list), len(act_target_list)\n",
    "        max_len = max(pred_len, target_len)\n",
    "        assert max_len > 0, \"Error: max_len should be greater than 0.\"\n",
    "\n",
    "        distance = damerau_levenshtein(act_pred_list, act_target_list)\n",
    "        normalized_distance = distance / max_len\n",
    "        total_dl_distance += normalized_distance\n",
    "\n",
    "        # -- MAE --\n",
    "\n",
    "        time_target_exp = time_suffix_exp[i, :target_len] # shape: (target_len,)\n",
    "        time_pred_exp = time_predictions_exp[i, :target_len] # shape: (target_len,)\n",
    "        mae_loss = time_criterion_1(time_target_exp, time_pred_exp)\n",
    "        total_mae +=  mae_loss.item()\n",
    "\n",
    "        # -- MSLE --\n",
    "\n",
    "        time_target = time_suffix[i, :target_len] # shape: (target_len,)\n",
    "        time_pred = time_predictions[i, :target_len] # shape: (target_len,)\n",
    "        msle_loss = time_criterion_2(time_target, time_pred)\n",
    "        total_msle +=  msle_loss.item() \n",
    "\n",
    "    # compute average metrics over the batch\n",
    "    dl_distance = total_dl_distance / batch_size\n",
    "    mae = total_mae / batch_size\n",
    "    msle = total_msle / batch_size\n",
    "    \n",
    "    return dl_distance, mae, msle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_distance, mae, msle = performance_metrics(act_predictions,\n",
    "                                             time_predictions,\n",
    "                                             test_suffix_act_tensor,\n",
    "                                             test_suffix_time_tensor,\n",
    "                                             max_pre,\n",
    "                                             min_pre)\n",
    "print(dl_distance)\n",
    "print(mae)\n",
    "print(msle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
