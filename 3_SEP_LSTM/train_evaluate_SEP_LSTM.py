"""
This module contains functions and a class for training a SEP-LSTM model to
predict the next event, as well as functions for evaluating the suffixes 
generated by the trained SEP-LSTM model.

Functions:
    train
    validate
    evaluate
    loss_function
    damerau_levenshtein
    performance_metrics
    init_weights

Class:
    EarlyStopper   
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np

def train(model, 
          dataloader,
          lr,
          device):
    """ 
    Train SEP-LSTM model.

    Parameters
    ----------
    model : torch.nn.Module
        An instance of the SEP_LSTM_trace or SEP_LSTM_cat class.
    dataloader : torch.utils.data.DataLoader
        Dataloader containing the training set data.
    lr : float
        Learning rate.
    device : torch.device
        Computation device (GPU or CPU).

    Returns
    -------
    avg_train_loss : float
        Total training loss, computed as the unweighted sum of the next activity 
        label and timestamp prediction losses.
    avg_train_act_loss : float
        Training loss for next activity label prediction.
    avg_train_time_loss : float
        Training loss for next timestamp prediction.
    """
    model.train() 

    # define optimizer
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # initialize losses
    train_epoch_loss, train_epoch_act_loss, train_epoch_time_loss = 0.0, 0.0, 0.0

    for batch in dataloader:

        # load data
        train_log_prefix, train_trace_prefix, train_next_act, train_next_time = batch
        # train_log_prefix shape: (batch_size, log_prefix_len, num_act + 1)
        # train_trace_prefix shape: (batch_size, trace_prefix_len, num_act + 2)
        # train_next_act shape: (batch_size,)
        # train_next_time shape: (batch_size,)
        
        train_log_prefix = train_log_prefix.float().to(device)
        train_trace_prefix = train_trace_prefix.float().to(device)
        train_next_time = train_next_time.float().to(device)
        train_next_act = train_next_act.long().to(device)
        
        # set the gradient to zero
        optimizer.zero_grad()

        # run a forward pass and obtain predictions
        act_prediction, time_prediction = model(train_log_prefix,
                                                train_trace_prefix)
        # act_prediction shape: (batch_size, num_act)
        # time_prediction shape: (batch_size, 1)

        # ensure predictions are stored in device
        act_prediction, time_prediction = act_prediction.to(device), time_prediction.to(device)
        
        # calculate act_loss, time_loss and loss
        loss, act_loss, time_loss = loss_function(act_prediction,
                                                 time_prediction,
                                                 train_next_act,
                                                 train_next_time)

        # backpropagation
        loss.backward()
        optimizer.step()

        # sum up losses from all batches
        train_epoch_loss += loss.item()
        train_epoch_act_loss += act_loss.item()
        train_epoch_time_loss += time_loss.item()
    
    # compute average losses over the batch
    avg_train_loss = train_epoch_loss / len(dataloader)
    avg_train_act_loss = train_epoch_act_loss / len(dataloader)
    avg_train_time_loss = train_epoch_time_loss / len(dataloader)
    
    return avg_train_loss, avg_train_act_loss, avg_train_time_loss

def validate(model,
            dataloader,
            device):
    """
    Calculate loss and performance metrics on the validation set for the SEP-LSTM
    model.

    Parameters
    ----------
    model : torch.nn.Module
        An instance of the SEP_LSTM_trace or SEP_LSTM_cat class.
    dataloader : torch.utils.data.DataLoader
        Dataloader containing the validation set data.
    device : torch.device
        Computation device (GPU or CPU).

    Returns
    -------
    avg_val_loss : float
        Total validation loss, computed as the unweighted sum of the next 
        activity label and timestamp prediction losses.
    avg_val_act_loss : float
        Validation loss for next activity label prediction.
    avg_val_time_loss : float
        Validation loss for next timestamp prediction.
    """
    model.eval()

    # initialize losses
    val_epoch_loss, val_epoch_act_loss, val_epoch_time_loss = 0.0, 0.0, 0.0

    with torch.no_grad():

        for batch in dataloader:

            # load data
            log_prefix, trace_prefix, next_act, next_time = batch
            # log_prefix shape: (batch_size, log_prefix_len, num_act + 1)
            # trace_prefix shape: (batch_size, trace_prefix_len, num_act + 2)
            # next_act shape: (batch_size,)
            # next_time shape: (batch_size,)

            log_prefix = log_prefix.float().to(device)
            trace_prefix = trace_prefix.float().to(device)
            next_time = next_time.float().to(device)
            next_act = next_act.to(torch.long).to(device)

            # run a forward pass and obtain predictions
            act_prediction, time_prediction = model(log_prefix,
                                                    trace_prefix)
            # act_prediction shape: (batch_size, num_act)
            # time_prediction shape: (batch_size, 1)

            # ensure predictions are stored in device
            act_prediction, time_prediction = act_prediction.to(device), time_prediction.to(device)

            # calculate losses
            val_loss, val_act_loss, val_time_loss = loss_function(act_prediction,
                                                                  time_prediction,
                                                                  next_act,
                                                                  next_time)

            # sum up losses from all batches
            val_epoch_loss += val_loss.item()
            val_epoch_act_loss += val_act_loss.item()
            val_epoch_time_loss += val_time_loss.item()

    # compute average losses over the batch
    avg_val_loss = val_epoch_loss / len(dataloader)
    avg_val_act_loss = val_epoch_act_loss / len(dataloader)
    avg_val_time_loss = val_epoch_time_loss / len(dataloader)
    
    return avg_val_loss, avg_val_act_loss, avg_val_time_loss

def evaluate(model, 
             dataloader,
             max_pre,
             min_pre,
             max_start,
             min_start,
             device):
    """
    Compute performance metrics for suffix predictions generated iteratively on 
    the test set using the SEP-LSTM model.

    Parameters
    ----------
    model : torch.nn.Module
        An instance of the SEP_LSTM_trace or SEP_LSTM_cat class.
    dataloader : torch.utils.data.DataLoader
        Dataloader containing the test set data.
    max_pre : float
        Maximum of the log-normalized trace_ts_pre.
    min_pre : float
        Minimum of the log-normalized trace_ts_pre.
    max_start : float
        Maximum of the log-normalized trace_ts_start.
    min_start : float
        Minimum of the log-normalized trace_ts_start.
    device : torch.device
        Computation device (GPU or CPU).

    Returns
    -------
    avg_test_dl_distance : float
        Normalized Damerau-Levenshtein distance for activity label suffix 
        prediction on the test set.
    avg_test_mae : float
        Mean Absolute Error (MAE) for timestamp suffix prediction on the test
        set.
    avg_test_msle: float
        Mean Squared Logarithmic Error (MSLE) for timestamp suffix prediction on 
        the test set.
    """ 
    model.eval()

    # initialize performance metrics
    test_epoch_dl_distance, test_epoch_mae, test_epoch_msle  = 0.0, 0.0, 0.0
    
    with torch.no_grad():

        for batch in dataloader:

            # load data
            log_prefix, trace_prefix, act_suffix, time_suffix = batch
            # log_prefix shape: (batch_size, log_prefix_len, num_act + 1)
            # trace_prefix shape: (batch_size, trace_prefix_len, num_act + 2)
            # act_suffix shape: (batch_size, suffix_len)
            # time_suffix shape: (batch_size, suffix_len)

            log_prefix = log_prefix.float().to(device)
            trace_prefix = trace_prefix.float().to(device)
            time_suffix = time_suffix.float().to(device)
            act_suffix = act_suffix.to(torch.long).to(device)

            # iteratively generate suffix predictions
            batch_size = trace_prefix.shape[0]
            num_act = log_prefix.shape[-1] - 1
            suffix_len = act_suffix.shape[-1]

            act_predictions = torch.zeros(batch_size, suffix_len, num_act)
            time_predictions = torch.zeros(batch_size, suffix_len, 1)

            trace_prefix_renew = trace_prefix.clone()

            for i in range(suffix_len):
                
                next_act, next_time_pre_2d = model(log_prefix, trace_prefix_renew)
                # next_act shape: (batch_size, num_act)
                # next_time_pre_2d shape: (batch_size, 1)

                act_predictions[:, i, :] = next_act
                time_predictions[:, i, :] = next_time_pre_2d

                next_time_pre = next_time_pre_2d.squeeze(-1) # shape: (batch_size, )

                # derive trace_ts_start from predicted TTNE
                old_next_time_start = trace_prefix_renew[:, -1, -2] # shape: (batch_size, )
                old_next_time_start = old_next_time_start* (max_start - min_start) + min_start
                old_next_time_start = torch.expm1(old_next_time_start)
                next_time_pre_denorm = next_time_pre* (max_pre - min_pre) + min_pre
                next_time_pre_denorm = torch.expm1(next_time_pre_denorm)
                next_time_start = old_next_time_start + next_time_pre_denorm
                next_time_start = torch.maximum(next_time_start, torch.tensor(0.0, device=device)) 
                next_time_start = torch.log1p(next_time_start)
                next_time_start = (next_time_start - min_start) / (max_start - min_start) # shape: (batch_size, )

                act_labels = next_act.argmax(1) # shape: (batch_size,)
                act_hot = F.one_hot(act_labels, num_classes=num_act)
                act_hot[:, 0] = 0 # shape: (batch_size, num_act)
                act_hot = act_hot.float()

                trace_prefix_renew[:, :-1, :] = trace_prefix_renew[:, 1:, :]
                trace_prefix_renew[:, -1, -(num_act+2):-2] = act_hot
                trace_prefix_renew[:, -1, -2] = next_time_start
                trace_prefix_renew[:, -1, -1] = next_time_pre

            # calculate performance metrics
            dl_distance, mae, msle = performance_metrics(act_predictions,
                                                         time_predictions,
                                                         act_suffix,
                                                         time_suffix,
                                                         max_pre,
                                                         min_pre)
            
            # sum up losses from all batches
            test_epoch_dl_distance += dl_distance
            test_epoch_mae += mae
            test_epoch_msle += msle

    # compute average metrics over the batch
    avg_test_dl_distance = test_epoch_dl_distance / len(dataloader)
    avg_test_mae =  test_epoch_mae / len(dataloader)
    avg_test_msle =  test_epoch_msle / len(dataloader)

    return avg_test_dl_distance, avg_test_mae, avg_test_msle

def loss_function(act_prediction, 
                  time_prediction, 
                  next_act,
                  next_time):
    """
    Calculate cross-entropy loss for next activity label prediction and Mean 
    Absolute Error (MAE) for next timestamp prediction. The final loss is the 
    unweighted sum of both losses.

    Parameters
    ----------
    act_prediction : torch.Tensor
        Next activity label prediction.
        Shape: (batch_size, num_act)
    time_predictions : torch.Tensor
        Next timestamp prediction.
        Shape: (batch_size, 1)
    next_act : torch.Tensor
        Ground truth next activity label.  
        Shape: (batch_size, )
    next_time : torch.Tensor
        Ground truth next timestamp.  
        Shape: (batch_size, )

    Returns
    -------  
    act_loss : torch.Tensor
        A scalar representing next activity label prediction loss, averaged 
        over each element in the batch.
    time_loss : torch.Tensor
        A scalar representing next timestamp prediction loss, averaged over 
        each element in the batch.
    loss : torch.Tensor
        A scalar representing the final loss.  
        loss = 0.5 * act_loss + 0.5 * time_loss

    """
    # define loss functions
    act_criterion = nn.CrossEntropyLoss()
    time_criterion = nn.L1Loss()

    # calculate act loss
    act_loss = act_criterion(act_prediction, next_act)

    # calculate time loss
    next_time = next_time.unsqueeze(-1) # shape: (batch_size, 1)
    time_loss = time_criterion(time_prediction, next_time)

    # calculate overall loss
    loss = 0.5 * act_loss + 0.5 * time_loss

    return loss, act_loss, time_loss

def damerau_levenshtein(list1, 
                        list2):
    """
    Compute the Damerau-Levenshtein distance between two sequences of activity labels.

    The Damerau-Levenshtein distance measures the minimum number of operations 
    (insertions, deletions, substitutions, and transpositions of adjacent elements) 
    required to transform one list into the other.

    Parameters
    ----------
    list1 : list
        First sequence of activity labels.
    list2 : list
        Second sequence of activity labels.

    Returns
    -------
    dl_distance : float
        The computed Damerau-Levenshtein distance between the two sequences.
    """
    len_1, len_2 = len(list1), len(list2)

    dist = [[0 for _ in range(len_2 + 1)] for _ in range(len_1 + 1)]

    for i in range(len_1 + 1):
        dist[i][0] = i
    for j in range(len_2 + 1):
        dist[0][j] = j

    for i in range(1, len_1 + 1):
        for j in range(1, len_2 + 1):
            cost = 0 if list1[i - 1] == list2[j - 1] else 1

            dist[i][j] = min(
                dist[i - 1][j] + 1,    # deletion
                dist[i][j - 1] + 1,    # insertion
                dist[i - 1][j - 1] + cost  # substitution
            )

            if i > 1 and j > 1 and list1[i - 1] == list2[j - 2] and list1[i - 2] == list2[j - 1]:
                dist[i][j] = min(
                    dist[i][j],
                    dist[i - 2][j - 2] + cost  # transposition
                )

    dl_distance = dist[len_1][len_2]

    return dl_distance

def performance_metrics(act_predictions, 
                        time_predictions, 
                        act_suffix,
                        time_suffix,
                        max_value,
                        min_value,
                        eoc_index = int(3)):
    """
    Compute three performance metrics for suffix prediction:  
    - dl_distance: Normalized Damerau-Levenshtein distance between predicted and 
      ground truth activity label suffixes.
    - mae: Mean Absolute Error between predicted and ground truth TTNE suffixes.
    - msle: Mean Squared Logarithmic Error between predicted and ground truth 
      TTNE suffixes.
    
    Parameters
    ----------
    act_predictions : torch.Tensor
        Activity label suffix predictions.  
        Shape: (batch_size, suffix_len, num_act)
    time_predictions : torch.Tensor
        Timestamp suffix predictions.  
        Shape: (batch_size, suffix_len, 1)
    act_suffix : torch.Tensor
        Ground truth activity label suffix.  
        Shape: (batch_size, suffix_len)
    time_suffix : torch.Tensor
        Ground truth timestamp suffix.  
        Shape: (batch_size, suffix_len)
    max_value : float
        Maximum of the log-normalized TTNE.
    min_value : float
        Minimum of the log-normalized TTNE.
    eoc_index : int
        Index representing the End-Of-Case (EOC) token in the activity label 
        vocabulary. 
        Default is 3.
    
    Returns
    -------
    dl_distance : float
        Average normalized Damerau-Levenshtein distance over the batch.   
    mae : float
        Average MAE over the batch.
    msle : float
        Average MSLE over the batch.
    """
    # get batch_size
    batch_size = act_predictions.shape[0]

    # convert predicted activity label probabilities to lebel indices
    act_predictions = act_predictions.argmax(2) # shape: (batch_size, suffix_len)

    # reshape time_predictions
    time_predictions = time_predictions.squeeze(-1) # shape: (batch_size, suffix_len)

    # de-normalize and reverse log transformation of TTNE
    time_suffix = time_suffix * (max_value - min_value) + min_value
    time_suffix_exp = torch.exp(time_suffix) - 1   
    time_predictions = time_predictions * (max_value - min_value) + min_value
    time_predictions_exp = torch.exp(time_predictions) - 1

    # initialize performance metrics
    total_dl_distance, total_mae, total_msle = 0.0, 0.0, 0.0

    # define loss functions
    time_criterion_1 = nn.L1Loss()
    time_criterion_2 = nn.MSELoss()

    for i in range(batch_size):

        # -- normalized Damerau-Levenshtein distance --

        # convert predicted and target activity label sequences to lists, 
        # truncated at EOC
        act_pred_list = []
        act_target_list = []

        for p in act_predictions[i].tolist():
            act_pred_list.append(p)
            if p == eoc_index: # stop at EOC token (inclusive)
                break    
            
        for t in act_suffix[i].tolist():
            act_target_list.append(t)
            if t == eoc_index: # stop at EOC token (inclusive)
                break
        
        # compute normalized Damerau-Levenshtein distance
        pred_len, target_len= len(act_pred_list), len(act_target_list)
        max_len = max(pred_len, target_len)
        assert max_len > 0, "Error: max_len should be greater than 0."

        distance = damerau_levenshtein(act_pred_list, act_target_list)
        normalized_distance = distance / max_len
        total_dl_distance += normalized_distance

        # -- MAE --

        time_target_exp = time_suffix_exp[i, :target_len] # shape: (target_len,)
        time_pred_exp = time_predictions_exp[i, :target_len] # shape: (target_len,)
        mae_loss = time_criterion_1(time_target_exp, time_pred_exp)
        total_mae +=  mae_loss.item()

        # -- MSLE --

        time_target = time_suffix[i, :target_len] # shape: (target_len,)
        time_pred = time_predictions[i, :target_len] # shape: (target_len,)
        msle_loss = time_criterion_2(time_target, time_pred)
        total_msle +=  msle_loss.item() 

    # compute average metrics over the batch
    dl_distance = total_dl_distance / batch_size
    mae = total_mae / batch_size
    msle = total_msle / batch_size
    
    return dl_distance, mae, msle

class EarlyStopper:
    """
    Implements early stopping to terminate training when validation performance 
    stops improving.

    Inspired by: https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch

    Parameters
    ----------
    patience : int
        Number of epochs with no significant improvement in validation metrics 
        before stopping.

    Attributes
    ----------
    patience : int
        The number of epochs with no significant improvement in validation 
        metrics before stopping.
    counter : int
        The current number of consecutive epochs without improvement.
    min_val_loss : float
        The minimum validation loss observed.

    Methods
    -------
    early_stop(val_loss) -> bool
        Checks if training should be stopped.
    """
    def __init__(self, patience):
        self.patience = patience
        self.counter = 0
        self.min_val_loss = float('inf')

    def early_stop(self, val_loss):
        """
        Determine whether to stop training early based on validation loss.

        Parameters
        ----------
        val_loss : float
            Current epoch's validation loss.

        Returns
        -------
        bool
            True if training should stop early, False otherwise.
        """
        improvement = False
        
        if val_loss < (self.min_val_loss-0.001):
            self.min_val_loss = val_loss
            improvement = True
        
        if improvement:
            self.counter = 0
        else:
            self.counter += 1

        if self.counter >= self.patience:
            return True
        
        return False
    
def init_weights(m):
    """
    Initialize model parameters uniformly in the range [-0.08, 0.08].

    Parameters
    ----------
    m : torch.nn.Module
        An instance of the Seq2Seq_trace or Seq2Seq_cat class.
    """
    for name, param in m.named_parameters():
        nn.init.uniform_(param.data, -0.08, 0.08)